# Case Study Understanding Kafka Architecture at LinkedIn

	- “If data is the lifeblood of high technology,  Apache Kafka is the circulatory system in use at LinkedIn. 
	- We use Kafka for moving every type of data around between systems, and it touches virtually every server, every day. 
	- The complexity of the infrastructure, as well as the reasoning behind choices that have been made in its implementation, 
	- has developed out of a need to move a large amount of data around quickly and reliably.”

# Scala of Kafka Ecosystem at LinkedIn

    	- 800 Billion Messages Per Day
    	- 175 TB of Data
    	- 650 TB Messages consumed per day
    	- 13 Million Messages per second
    	- 2.75 GB Messages Per Second
    	- 1100 Kafka Brokers organized in 60 Clusters

# LinkedIn has categorized their type of messages into 4 categories:

    	- Queuing: 
		    - Messages are produced by one part of an application and consumed by another part of that same application. 
		    - This type of message is used for sending out emails, distributing data sets that are computed by another online application, or coordinating with a backend component.
    	- Metrics: 
		    - This handles all measurements generated by applications in their operation. It includes everything from OS and hardware statistics to application-specific measurements which are critical to ensuring the proper functioning of the system. 
		    - The alerting and internal monitoring systems are driven by this.
    	- Logs: 
		    - This includes application, system, and public access logs.  
		    - The logging data is produced into Kafka by applications, and then read by other systems for log aggregation purposes.
    	- Tracking Data: 
		    - This includes every action taken on the front lines of LinkedIn’s infrastructure, whether by users or applications. 
		    - The data is processed by Apache Samza for stream processing and Apache Hadoop for batch processing.


    	- LinkedIn operates out of multiple datacenters. 
	    - Some applications, such as those serving a specific user’s requests, are only concerned with what is going on in a single datacenter. 
	    - Many other applications, such as those maintaining the indices that enable search, need a view of what is going on in all datacenters.

    	- For each message category, LinkedIn has a cluster named local containing messages created in the datacenter. 
	    - There is also an aggregate cluster, which combines messages from all local clusters for a given category. 
	    - Kafka mirror maker application copies messages forward, from local into aggregate.

    	- The producer is the first tier, the local cluster (across all datacenters) is the second, and each of the aggregate clusters is an additional tier. 
	    - The consumer itself is the final tier.

    	- Although in an individual kafka cluster, messages are generally not lost, when we include the tiered architecture, it creates a lot of points where messages can disappear.

    	- Producers & Consumers also periodically send an audit message about how many messages they have produced or consumed. We can tally the counts to see if any messages are missing.

    	- The Kafka Console Auditor, consumes all messages from all topics in a single Kafka cluster. 
	    - It periodically sends how many messages it received in the last interval. 
	    - We can tally the producer counts with the audit messages to see if there is any data loss.

    	- The Audit messages are consumed and stored in an Audit DB and rendered through Audit UI for reporting



	