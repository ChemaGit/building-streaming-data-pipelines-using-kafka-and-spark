## INGESTING WEB SERVER LOGS TO KAFKA
````text
- As we understood Kafka in detail let us see how we can get data from web server logs into Kafka topic.
- We can either use APIs as part of the application to directly produce messages to Kafka topic or use plugins like:
- Flume, Kafka Connect etc to get data from log files generated by web applications.
    - Getting data into Kafka topic
    - Recap of Flume
    - Ingest data into Kafka topic from Flume
    - Ingest data into Kafka topic using Kafka Connect
    - Consuming data from Kafka topic
````

### Getting data into Kafka topic
````text
- Kafka topic is intermediate data structure.
- We can get data into Kafka topic using different approaches.
    - Custom applications using Producer API.
    - Kafka Connect.
    - Logtash
    - Flume
    - and more

- Producer APIs can be used as part of the web applications to push log messages to Kafka topic directly.
- But in some cases it might not be possible to use Producer APIs (e.g.: mission critical legacy web applications)
- We need to use tools like Logstash, Flume etc.
````

### Flume
````text
- Flume can be used to read log messages from log files and ingest data into different types of sinks.
- One of them is Kafka
- Channalize your data using Flume
- As there are multiple log files, we need to have one agent per log file
- Flume agents can be used to ingest data from different sources to different sinks
- Each Flume agent is combination of source, sink and channel
- We can configure Flume agents with different strategies - multi agent linear flow, multiplexing, replicating, consolidation, etc..
- Sample agent to replicate data from log file to HDFS and Kafka
````

### Ingest data into Kafka topic from Flume
````text
- Here are the steps to get data into Kafka topic from Flume.
- We will use multiplexing and replicate data into both HDFS as well as Kafka topic.
    - Define source type - exec or spooldir
    - Define sinks - HDFS  or Kafka
    - Define channels - Memory or File
    - Run Flume agent and make sure data gets into Kafka
    - Use kafka-console-consumer to confirm data is coming into Kafka
````

### multi-sink.conf: A single-node Flume configuration 
```properties
# Name the components on this agent
a1.sources = r1
a1.sinks = k1 k2
a1.channels = c1 c2

# Describe/configure the source
a1.sources.r1.type = exec
a1.sources.r1.command = tail -F /opt/gen_logs/logs/access.log

# Describe the sink
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = hdfs://quickstart.cloudera/user/cloudera/flume/flumekafka
a1.sinks.k1.hdfs.filePrefix = retail
a1.sinks.k1.hdfs.fileSuffix = .txt
a1.sinks.k1.hdfs.rollInterval = 60
a1.sinks.k1.hdfs.rollSize = 0
a1.sinks.k1.hdfs.rollCount = 100
a1.sinks.k1.hdfs.fileType = DataStream
a1.sinks.k2.type = org.apache.flume.sink.kafka.KafkaSink
a1.sinks.k2.kafka.bootstrap.servers = quickstart.cloudera:9092
a1.sinks.k2.kafka.topic = flume-kafka-retail

#Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100
a1.channels.c2.type = memory
a1.channels.c2.capacity = 1000
a1.channels.c2.transactionCapacity = 100

#Bind the source and sink to the channel
a1.sources.r1.channels = c1 c2
a1.sinks.k1.channel = c1
a1.sinks.k2.channel = c2
```

````bash
$ gedit /home/cloudera/flume_demo/flumekafka.conf &
$ kafka-topics --zookeeper quickstart.cloudera:2181 --create --topic flume-kafka-retail --partitions 3 --replication-factor 2
$ kafka-topics --zookeeper quickstart.cloudera:2181 --describe --topic flume-kafka-retail
$ flume-ng agent --name a1 --conf-file /home/cloudera/flume_demo/multi-sink.conf
$ kafka-console-consumer --bootstrap-server quickstart.cloudera:9092 --topic flume-kafka-retail
$ hdfs dfs -ls /user/cloudera/flume/flumekafka
````

### Ingest data into Kafka topic using Kafka Connect
````text
- We can also use Kafka connect to get data from log files into Kafka topic
    - We can either use standalone or distributed mode for Kafka connect worker
    - Examples are available under $KAFKA_CONF_DIR
    - Kafka support bunch of sources and sinks as part of connect
    - To get data from log files we need to use file as source
    - Example for file source is available under: 
    - $KAFKA_CONF_DIR/connect-file-source.properties
    - We can start the worker by passing 2 arguments - one worker mode and other source type.
    - We can validate by consuming data using kafka-console-consumer
````
````bash
$ mkdir kafkaconnectdemo
$ cd kafkaconnectdemo
$ cp /opt/cloudera/parcels/KAFKA-4.0.0-1.4.0.0.p0.1/etc/kafka/conf.dist/connect-standalone.properties /home/cloudera/flume_demo/kafkaconnectdemo
$ gedit connect-standalone.properties

# we override some properties
# bootstrap.servers=quickstart.cloudera:9092
		
$ cp /opt/cloudera/parcels/KAFKA-4.0.0-1.4.0.0.p0.1/etc/kafka/conf.dist/connect-file-source.properties /home/cloudera/flume_demo/kafkaconnectdemo
$ gedit connect-file-source.properties

# we override some properties
    # file=/opt/gen_logs/logs/access.log
    # topic=kafka-connect-retail
    
# Let's create a kafka topic
$ kafka-topics --zookeeper quickstart.cloudera:2181 --create --topic kafka-connect-retail --partitions 4 --replication-factor 1
# Let's run kafka-connect and the consumer
$ ./opt/cloudera/parcels/KAFKA-4.0.0-1.4.0.0.p0.1/lib/kafka/bin/connect-standalone.sh /home/cloudera/flume_demo/kafkaconnectdemo/connect-standalone.properties /home/cloudera/flume_demo/kafkaconnectdemo/connect-file-source.properties
$ kafka-console-consumer --bootstrap-server quickstart.cloudera:9092 --topic kafka-connect-retail --from-beginning
````

### Consuming data from Kafka topic
````text
- Once data is in Kafka topic, we can consume data using different approaches.
- Approach is determined based up on the requirements
    - Kafka connect
    - Kafka Streams or Kafka SQL
    - Flume
    - Spark Streaming or Storm or Flink
    - Kafka connect and Flume are used for replicating data as is with no or minimum row level transformations
    - If we have to apply transformations such as aggregations we need to use Kafka Streams 
      or Kafka SQL or any other streaming analytics tools such as Spark Streaming.
````